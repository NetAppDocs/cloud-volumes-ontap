---
sidebar: sidebar
permalink: reference_storage_limits_94.html
keywords: limits, maximum, storage, aggregates, disks, volumes, capacity, luns, size, storage virtual machine, SVM
summary: Cloud Volumes ONTAP has storage configuration limits to provide reliable operations. For best performance, do not configure your system at the maximum values.
---

= Storage limits for Cloud Volumes ONTAP 9.4
:toc: macro
:hardbreaks:
:toclevels: 1
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
Cloud Volumes ONTAP has storage configuration limits to provide reliable operations. For best performance, do not configure your system at the maximum values.

The following sections list limits for aggregates, volumes, LUNs, and related storage objects. Note that the maximum capacity is model specific. Cloud Volumes ONTAP configurations that support a lower raw capacity limit cannot reach some of the size and disk limits. All limits are per Cloud Volumes ONTAP node.

toc::[]

== System capacity limit

A system's capacity limit depends on the Cloud Volumes ONTAP license that you choose for the system:

[cols="40,60",width=65%,options="header"]
|===
| License
| Capacity limit

| Explore	| 2 TB
| Standard | 10 TB
| Premium .2+a|
* AWS: 368 TB (360 TB for HA)
* Azure: 252 TB
| BYOL |

|===

Notes:

. If you enable data tiering, a system's capacity limit stays the same. The capacity limit includes both disks and object storage.

. In AWS, an HA pair's mirrored data does not count against the system capacity limit.
+
Data in an HA pair is synchronously mirrored between the nodes so that the data is available in the event of failure. If you purchase an 8 TB disk on node A, Cloud Manager also allocates an 8 TB disk on node B that is used for mirrored data. While 16 TB of capacity was provisioned, only 8 TB counts against the license limit.

. In AWS, both nodes in an HA pair cannot reach the system capacity limit due to the AWS disk limit.
+
For example, if you allocated 23 disks to node A for a total of 360 TB, 23 mirrored disks would also be allocated to node B. That would leave room for 9 disks on node B before reaching the disk limit.

== Aggregate and disk limits for Cloud Volumes ONTAP in AWS

[cols=3*,options="header,autowidth"]
|===
| Physical storage
| Parameter
| Limit

.5+| *Aggregates and disks*
| Maximum number of aggregates | 35 for single-node configurations
18 per node in an HA configuration ^1^
| Maximum aggregate size |	96 TB of raw capacity ^2^^,^ ^3^
| Minimum number of disks per aggregate	| 1
| Maximum number of disks per aggregate	| 6
| Maximum number of data disks across all aggregates ^4^ | 35 for single-node configurations
32 per node in an HA configuration
| *RAID groups*	| Maximum per aggregate	| 1
|===

Notes:

. It is not possible to create 18 aggregates on both nodes in an HA pair because doing so would exceed the data disk limit.

. The maximum usable capacity of a 96 TB aggregate is 85.05 TB.

. The aggregate capacity limit is based on the disks that comprise the aggregate. The limit does not include object storage used for data tiering.

. The data disk limit is specific to disks that contain user data. The boot disk and root disk for each node are not included in this limit.

== Aggregate and disk limits for Cloud Volumes ONTAP in Azure

[cols=3*,options="header,autowidth"]
|===
| Physical storage
| Parameter
| Limit

.5+| *Aggregates and disks*
| Maximum number of aggregates | 63
| Maximum aggregate size |	48 TB of raw capacity ^1^^,^ ^2^
| Minimum number of disks per aggregate	| 1
| Maximum number of disks per aggregate	| 12
| Maximum number of data disks across all aggregates ^3^ | 63
| *RAID groups*	| Maximum per aggregate	| 1
|===

Notes:

. The maximum usable capacity of a 48 TB aggregate is 42.52 TB.

. The aggregate capacity limit is based on the disks that comprise the aggregate. The limit does not include object storage used for data tiering.

. The data disk limit is specific to disks that contain user data. The boot disk and root disk for each node are not included in this limit.

== Logical storage limits

[cols=3*,options="header,autowidth"]
|===
| Logical storage
| Parameter
| Limit

| *Storage virtual machines (SVMs)*	| Maximum per node | One data-serving SVM and one or more SVMs used for disaster recovery ^1^
.2+| *Files*	| Maximum size | Volume size dependent
| Maximum per volume |	Volume size dependent, up to 2 billion
| *FlexClone volumes*	| Hierarchical clone depth ^2^ | 499
.3+| *FlexVol volumes*	| Maximum per node |	500
| Minimum size |	20 MB
| Maximum size |	AWS: 76.55 TB of fully provisioned capacity ^3^
Azure: 38.27 TB of fully provisioned capacity ^3^
| *Qtrees* |	Maximum per FlexVol volume |	4,995
| *Snapshot copies* |	Maximum per FlexVol volume |	1,023

|===

Notes:

. Cloud Manager does not provide any setup or orchestration support for SVM disaster recovery. It also does not support storage-related tasks on any additional SVMs. You must use System Manager or the CLI for SVM disaster recovery.

. Hierarchical clone depth is the maximum depth of a nested hierarchy of FlexClone volumes that can be created from a single FlexVol volume.

. The specified limit is based on the assumption that you use Cloud Manager to create a volume and that you keep the aggregate free space ratio at 10 percent, which Cloud Manager sets by default.

== iSCSI storage limits

[cols=3*,options="header,autowidth"]
|===
| iSCSI storage
| Parameter
| Limit

.4+| *LUNs*	| Maximum per node |	1,024
| Maximum number of LUN maps |	1,024
| Maximum size	| 16 TB
| Maximum per volume	| 512
| *igroups*	| Maximum per node | 256
.2+| *Initiators*	| Maximum per node |	512
| Maximum per igroup	| 128
| *iSCSI sessions* |	Maximum per node | 1,024
.2+| *LIFs*	| Maximum per port |	32
| Maximum per portset	| 32
| *Portsets* |	Maximum per node |	256

|===
